{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOitzPkRhAyKDgbEiWhgUCT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilesh22210374/gen_ai/blob/main/GenAI_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H503JTkQ3fu",
        "outputId": "f312abe1-2e4b-4eb0-8577-aae9f8f22f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: How are you?\n",
            "\n",
            "Interview Approach:\n",
            "Can you rephrase or clarify the question: 'How are you?'? How are you\n",
            "What are the key sub-questions to answer How are you? (Separate each sub-question by a comma) How are you\n",
            "Provide your answer for subquestion 'How are you': I am fine\n",
            "Answer to How are you: I am fine\n",
            "\n",
            "\n",
            "Chain of Thought:\n",
            "Question: How are you?\n",
            "Enter the final answer considering the reasoning steps: fine\n",
            "Let's break this down step by step.\n",
            "First we need to consider...\n",
            "Then we need to take into account...\n",
            "Based on these factors, we can conclude...\n",
            "Final Answer:fine\n",
            "\n",
            "Tree of Thought:\n",
            "Question: How are you?\n",
            "What's the outcome of Option 1: ok\n",
            "What's the outcome of Option 2: fine\n",
            "What's the outcome of Option 3: Nice\n",
            "Enter the final answer based on the different options provided: i am fine\n",
            "Option 1: ... -> ok\n",
            "Option 2: ... -> fine\n",
            "Option 3: ... -> Nice\n",
            "Final Answer (choose the best option):i am fine\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def interview_approach(question):\n",
        "  \"\"\"\n",
        "  Simulates an interview approach to prompt engineering.\n",
        "\n",
        "  Args:\n",
        "      question: The question to be answered.\n",
        "\n",
        "  Returns:\n",
        "      A string representing the answer.\n",
        "  \"\"\"\n",
        "  clarified_question = input(f\"Can you rephrase or clarify the question: '{question}'? \")\n",
        "  if clarified_question:\n",
        "      question = clarified_question\n",
        "\n",
        "  sub_questions = input(f\"What are the key sub-questions to answer {question}? (Separate each sub-question by a comma) \")\n",
        "\n",
        "  if sub_questions:\n",
        "      sub_questions = sub_questions.split(',')\n",
        "      answer = \"\"\n",
        "      for sub_q in sub_questions:\n",
        "          answer += f\"Answer to {sub_q.strip()}: \"\n",
        "          answer += input(f\"Provide your answer for subquestion '{sub_q.strip()}': \") + \"\\n\"\n",
        "  else:\n",
        "      answer = input(f\"Please provide a direct answer to the question: '{question}' \")\n",
        "\n",
        "  return answer\n",
        "\n",
        "def chain_of_thought(question):\n",
        "    \"\"\"\n",
        "    Simulates a chain-of-thought prompting approach.\n",
        "\n",
        "    Args:\n",
        "        question: The question to prompt the model with.\n",
        "\n",
        "    Returns:\n",
        "        The model's response.\n",
        "    \"\"\"\n",
        "    print(f\"Question: {question}\")\n",
        "\n",
        "    reasoning_steps = [\n",
        "        \"Let's break this down step by step.\",\n",
        "        \"First we need to consider...\",\n",
        "        \"Then we need to take into account...\",\n",
        "        \"Based on these factors, we can conclude...\"\n",
        "    ]\n",
        "\n",
        "    response = \"\"\n",
        "    for step in reasoning_steps:\n",
        "        response += f\"{step}\\n\"\n",
        "    response += \"Final Answer:\"\n",
        "    response += input(\"Enter the final answer considering the reasoning steps: \")\n",
        "\n",
        "    return response\n",
        "\n",
        "def tree_of_thought(question):\n",
        "    \"\"\"\n",
        "    Simulates a tree-of-thought prompting approach.\n",
        "\n",
        "    Args:\n",
        "      question: The question to prompt the model with.\n",
        "\n",
        "    Returns:\n",
        "      The model's response.\n",
        "    \"\"\"\n",
        "    print(f\"Question: {question}\")\n",
        "\n",
        "    possible_paths = [\n",
        "      \"Option 1: ...\",\n",
        "      \"Option 2: ...\",\n",
        "      \"Option 3: ...\"\n",
        "    ]\n",
        "\n",
        "    response = \"\"\n",
        "    for path in possible_paths:\n",
        "        response += f\"{path} -> \"\n",
        "        response += input(f\"What's the outcome of {path.split(':')[0]}: \") + \"\\n\"\n",
        "\n",
        "    response += \"Final Answer (choose the best option):\"\n",
        "    response += input(\"Enter the final answer based on the different options provided: \")\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "user_question = input(\"Enter your question: \")\n",
        "\n",
        "print(\"\\nInterview Approach:\")\n",
        "print(interview_approach(user_question))\n",
        "\n",
        "print(\"\\nChain of Thought:\")\n",
        "print(chain_of_thought(user_question))\n",
        "\n",
        "print(\"\\nTree of Thought:\")\n",
        "print(tree_of_thought(user_question))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def zero_shot_prompt(question, task_description):\n",
        "    \"\"\"\n",
        "    Performs zero-shot prompting.\n",
        "\n",
        "    Args:\n",
        "      question: The question to ask the model.\n",
        "      task_description: A description of the task.\n",
        "\n",
        "    Returns:\n",
        "      The model's response.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"{task_description}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    print(prompt)\n",
        "    answer = input(\"Enter the answer based on the prompt: \")\n",
        "    return answer\n",
        "\n",
        "\n",
        "def few_shot_prompt(question, examples, task_description):\n",
        "    \"\"\"\n",
        "    Performs few-shot prompting.\n",
        "\n",
        "    Args:\n",
        "        question: The question to ask the model.\n",
        "        examples: A list of (input, output) examples.\n",
        "        task_description: A description of the task.\n",
        "\n",
        "    Returns:\n",
        "        The model's response.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"{task_description}\\n\\n\"\n",
        "    for input_example, output_example in examples:\n",
        "        prompt += f\"Input: {input_example}\\nOutput: {output_example}\\n\\n\"\n",
        "    prompt += f\"Input: {question}\\nOutput:\"\n",
        "    print(prompt)\n",
        "    answer = input(\"Enter the answer based on the prompt: \")\n",
        "    return answer\n",
        "\n",
        "user_question = input(\"Enter your question: \")\n",
        "\n",
        "task_desc_zero_shot = \"Answer the following question about animals.\"\n",
        "zero_shot_answer = zero_shot_prompt(user_question, task_desc_zero_shot)\n",
        "print(\"\\nZero-Shot Answer:\")\n",
        "print(zero_shot_answer)\n",
        "\n",
        "\n",
        "task_desc_few_shot = \"Translate English phrases to French.\"\n",
        "examples = [\n",
        "    (\"Hello\", \"Bonjour\"),\n",
        "    (\"Goodbye\", \"Au revoir\"),\n",
        "    (\"Thank you\", \"Merci\"),\n",
        "]\n",
        "few_shot_answer = few_shot_prompt(user_question, examples, task_desc_few_shot)\n",
        "print(\"\\nFew-Shot Answer:\")\n",
        "few_shot_answer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "-01VL9aQRMEL",
        "outputId": "45a37c0b-955c-4174-cde5-81c3d788c115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: What are you doing\n",
            "Answer the following question about animals.\n",
            "\n",
            "Question: What are you doing\n",
            "Answer:\n",
            "Enter the answer based on the prompt: i am doing nothing\n",
            "\n",
            "Zero-Shot Answer:\n",
            "i am doing nothing\n",
            "Translate English phrases to French.\n",
            "\n",
            "Input: Hello\n",
            "Output: Bonjour\n",
            "\n",
            "Input: Goodbye\n",
            "Output: Au revoir\n",
            "\n",
            "Input: Thank you\n",
            "Output: Merci\n",
            "\n",
            "Input: What are you doing\n",
            "Output:\n",
            "Enter the answer based on the prompt: i am doing nothing\n",
            "\n",
            "Few-Shot Answer:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i am doing nothing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}